{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPcfHT5C-mKZ"
   },
   "source": [
    "# HW1: seq2seq nmt\n",
    "\n",
    "**Homework Goals**\n",
    "\n",
    "1. Get familiar with text data preparation\n",
    "2. Learn to work with RNN\n",
    "3. Train the model to translate `en-->ru`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T14:31:09.928030Z",
     "start_time": "2022-12-03T14:31:09.923047Z"
    },
    "id": "0hUc8aEg8S2_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "from typing import Union, Tuple, List, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdM5tBYV8S3J"
   },
   "source": [
    "## Naive way of texts representation:\n",
    "\n",
    "0. Normalize spelling\n",
    "1. Filter out all special characters\n",
    "2. Split by spaces, do *naive tokenization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T17:53:38.320982Z",
     "start_time": "2022-12-01T17:53:22.951619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet before preprocessing (size - 174):\n",
      "\n",
      "   ! \" $ % & ' ( ) + , - . / 0 1 2 3 4 5 6 7 8 9 : ; ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z   « ° º » ã ç é ê î ï ó ö ú ü ́ Ё А Б В Г Д Е Ж З И Й К Л М Н О П Р С Т У Ф Х Ц Ч Ш Щ Ь Э Ю Я а б в г д е ж з и й к л м н о п р с т у ф х ц ч ш щ ъ ы ь э ю я ё ׁ ​ – — ― ‘ ’ …   ‽ ₂ € № \n",
      "\n",
      "Alphabet after preprocessing (size - 62):\n",
      "  ! , . ? a b c d e f g h i j k l m n o p q r s t u v w x y z а б в г д е ж з и к л м н о п р с т у ф х ц ч ш щ ъ ы ь э ю я \n",
      "\n",
      "Pairs (few examples):\n",
      "('go .', 'иди .') ('go .', 'идите .') ('hi .', 'здравствуите .') ('hi .', 'привет !') ('hi .', 'хаи .') ('hi .', 'здрасте .') ('hi .', 'здорово !') ('run !', 'беги !') ('run !', 'бегите !') ('run .', 'беги !')\n",
      "Total pairs qantity: 336666\n"
     ]
    }
   ],
   "source": [
    "class SeqPreproc:\n",
    "    def __init__(self) -> None:\n",
    "        self.raw_alphabet = set()\n",
    "        self.alphabet = set()\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(seq: str):\n",
    "        return \"\".join(char for char in unicodedata.normalize('NFD', seq) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "    def preprocess(self, seq: str, add_sym: bool = True) -> str:\n",
    "        # adding raw symbols\n",
    "        if add_sym:\n",
    "            self.raw_alphabet.update(seq)\n",
    "        \n",
    "        # clean up sequence\n",
    "        seq = self.normalize(seq.lower().strip())\n",
    "        seq = re.sub(r\"[^a-zа-я?.,!]+\", \" \", seq)\n",
    "        seq = re.sub(r\"([.!?])\", r\" \\1\", seq)\n",
    "        \n",
    "        if add_sym:\n",
    "            self.alphabet.update(seq)\n",
    "        return seq\n",
    "\n",
    "\n",
    "with open(\"eng-rus.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    sp = SeqPreproc()\n",
    "    pairs = [tuple(map(sp.preprocess, line.split(\"\\t\"))) for line in f.readlines()]\n",
    "\n",
    "print(f\"Alphabet before preprocessing (size - {len(sp.raw_alphabet)}):\")\n",
    "print(*sorted(sp.raw_alphabet), \"\\n\")\n",
    "print(f\"Alphabet after preprocessing (size - {len(sp.alphabet)}):\")\n",
    "print(*sorted(sp.alphabet), \"\\n\")\n",
    "print(\"Pairs (few examples):\")\n",
    "print(*pairs[:10])\n",
    "print(f\"Total pairs qantity: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9ARXr6b8S3U"
   },
   "source": [
    "Each word will be assigned a number + we will need special tokens for the beginning and end of the sequence and for unknown words.\n",
    "`<SOS>, <EOS>, <UNK>`\n",
    "\n",
    "We have two languages, to work with each we need functions for translating from words to numbers and vice versa.\n",
    "\n",
    "It is proposed to implement these functions as dictionaries. Allocate the first 4 numbers for special tokens\n",
    "\n",
    "**(1 point)** Implement the dictionary building function, the function takes a list of strings (normalized sentences, can be splited by spaces) as input. Organize the dictionary in a reasonable way so that rare words can be thrown out if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T17:53:54.458498Z",
     "start_time": "2022-12-01T17:53:52.603298Z"
    },
    "id": "2epLOt_-8S3V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "COMMON_TOKENS = ['PAD', 'SOS', 'EOS', 'UNK']\n",
    "\n",
    "\n",
    "def build_vocabs(sents: tuple, max_size: int = 1000, special_tokens: list = None) -> Tuple[dict]:\n",
    "    vocab = dict()\n",
    "    \n",
    "    for seq in sents:\n",
    "        for token in seq.split():\n",
    "            vocab[token] = vocab.get(token, 0) + 1\n",
    "            \n",
    "    vocab = {token: qty for token, qty in sorted(vocab.items(), key=lambda tup: tup[1], reverse=True)}\n",
    "    tokens = special_tokens.copy() if special_tokens else list()\n",
    "    tokens.extend(list(islice(vocab.keys(), max_size - len(special_tokens))) )\n",
    "    tok2idx, idx2tok = \\\n",
    "        dict(zip(tokens, range(len(tokens)))), dict(zip(range(len(tokens)), tokens))\n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "\n",
    "eng, rus = list(zip(*pairs))\n",
    "rus2idx, idx2rus = build_vocabs(rus, max_size=10000, special_tokens=COMMON_TOKENS)\n",
    "eng2idx, idx2eng = build_vocabs(eng, max_size=5000 , special_tokens=COMMON_TOKENS)\n",
    "\n",
    "print(len(rus2idx), len(idx2rus))\n",
    "print(len(eng2idx), len(idx2eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T17:53:55.078217Z",
     "start_time": "2022-12-01T17:53:55.065167Z"
    },
    "id": "xh5koecS8S3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2539, 1264, 83, 2]\n",
      "SOS привет мир ! EOS\n",
      "[1, 1960, 439, 174, 2]\n",
      "SOS hello world ! EOS\n"
     ]
    }
   ],
   "source": [
    "def sentence2idx(seq: str, tok2idx: dict) -> list():\n",
    "    \"\"\"\n",
    "    Takes sentence as input and returns sequence of tokens indexes\n",
    "    \"\"\"\n",
    "    tokens = SeqPreproc().preprocess(seq=seq).split()\n",
    "    unk = tok2idx.get(\"UNK\")\n",
    "    return [tok2idx.get(\"SOS\")] + [tok2idx.get(token, unk) for token in tokens] + [tok2idx.get(\"EOS\")]\n",
    "\n",
    "\n",
    "def idx2sentence(seq: list, idx2tok: dict) -> str:\n",
    "    \"\"\"\n",
    "    Takes sequence of tokens indexes as input and returns sentence\n",
    "    \"\"\"\n",
    "    return \" \".join(idx2tok.get(idx) for idx in seq)\n",
    "\n",
    "# check the consistency of the transformations\n",
    "x = sentence2idx('Привет мир!', rus2idx)\n",
    "print(x)\n",
    "print(idx2sentence(seq=x, idx2tok=idx2rus))\n",
    "\n",
    "x = sentence2idx('Hello world!', eng2idx)\n",
    "print(x)\n",
    "print(idx2sentence(seq=x, idx2tok=idx2eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dhjjx52i8S3g"
   },
   "source": [
    "## Dealing with arbitrary length sequences in pytorch\n",
    "\n",
    "We need to be able to generate batches of `[bs, 1, seq_len]` tensors.\n",
    "But in our dataset, the samples are of different lengths:\n",
    "\n",
    "- we could cut everything down to the minimum length\n",
    "- padd to maximum length\n",
    "- choose some average length\n",
    "\n",
    "**(1 point)** Split the dataset on train and validate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T18:33:29.760207Z",
     "start_time": "2022-12-01T18:33:06.380148Z"
    },
    "id": "kmGX7wtL8S3i"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d50e9781314b4faeb6208a8ed040f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=336666.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train size: 269332 \n",
      "Eval size: 67334\n"
     ]
    }
   ],
   "source": [
    "# make a dataset with encoded pairs:\n",
    "class EngRusDataset(Dataset):\n",
    "    def __init__(self, pairs: List[tuple], pad_to: int = None, pad_value: int = 0, \n",
    "                 pad_left: bool = False) -> None:\n",
    "        self.pairs = pairs\n",
    "        self.pad_to = pad_to\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_left = pad_left\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def transform(self, seq: list) -> torch.tensor:\n",
    "        if self.pad_to is not None and len(seq) != self.pad_to:\n",
    "            if len(seq) > self.pad_to:\n",
    "                seq = seq[:self.pad_to]\n",
    "            else:\n",
    "                n_pads = self.pad_to - len(seq)\n",
    "                seq = [self.pad_value]*n_pads*self.pad_left + \\\n",
    "                      seq + \\\n",
    "                      [self.pad_value]*n_pads*(not self.pad_left)       \n",
    "        return torch.tensor(seq, dtype=torch.int)\n",
    "        \n",
    "    def __getitem__(self, item: int) -> Dict:\n",
    "        eng, rus = self.pairs[item]\n",
    "        eng, rus = self.transform(seq=eng), self.transform(seq=rus)\n",
    "        return dict(\n",
    "            eng=eng,\n",
    "            rus=rus,\n",
    "        )\n",
    "\n",
    "encoded = []\n",
    "for eng, rus in tqdm(pairs):\n",
    "    a = sentence2idx(eng, eng2idx)\n",
    "    b = sentence2idx(rus, rus2idx)\n",
    "    encoded.append((a, b))\n",
    "\n",
    "\n",
    "train_pairs, eval_pairs = train_test_split(encoded, train_size=0.8, random_state=42, shuffle=True)\n",
    "trainset = EngRusDataset(pairs=train_pairs, pad_to=8, pad_left=True)\n",
    "evalset = EngRusDataset(pairs=eval_pairs, pad_to=8, pad_left=True)\n",
    "\n",
    "print(\"Train size:\", trainset.__len__(), \"\\nEval size:\", evalset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PDpAtEq8S3n"
   },
   "source": [
    "Let's build a naive DataLoader and check how it makes batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T18:33:32.107452Z",
     "start_time": "2022-12-01T18:33:31.952404Z"
    },
    "id": "B-wLIzt88S3o"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "it = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T18:34:08.845645Z",
     "start_time": "2022-12-01T18:34:08.823645Z"
    },
    "id": "3CM0vzGL8S3s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    5,  140,   95,  381,    4,    2],\n",
       "        [   1,  131,  154,  476, 3074,   48,  491,  175],\n",
       "        [   0,    1,   22,   11,  472,  297,    4,    2],\n",
       "        [   1,    5,   41,    5,   49,  111,   13,    4],\n",
       "        [   1,    6,   86,   11,   29,   23,   28,  319],\n",
       "        [   1,    6,  255,   19,  311,   12,  547,    4],\n",
       "        [   0,    1,    5,  222,   11,  911,    4,    2],\n",
       "        [   1,    5,   22,   11,  259,   45,   63,   91]], dtype=torch.int32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(it)['eng']\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7blZtcq8S3w"
   },
   "source": [
    "In my case, the result was:\n",
    "```\n",
    "[tensor([1, 1, 1, 1, 1, 1, 1, 1]),\n",
    " tensor([ 6,  7,  6, 15,  5,  6,  5, 62]),\n",
    " tensor([ 48,  34,  83,   7,  32, 221,  22,  43]),\n",
    " tensor([  5, 143,  37,  36, 129,  12,  11,  66]),\n",
    " tensor([  73, 1258,  279,    8,    6,  555,   41,   10]),\n",
    " tensor([  8, 140,   8, 628,  20,  96,  13, 270]),\n",
    " tensor([  47,    4,   15,   18,   55,  269,    6, 1287]),\n",
    " tensor([ 58,   2,  13, 140, 193, 140, 171, 140])]\n",
    "```\n",
    "\n",
    "What's weird here?\n",
    "1. This is not a tensor, but a list of tensors. Accordingly, when iterating over zero dimension (`batch[i, :]`), we will get not an i-example, but i-tokens for all examples in the batch. This is not a problem, but different from the expected behavior.\n",
    "2. Only one example ends with `<EOS>` (2), the others are cut off to match its length. And this is a problem.\n",
    "\n",
    "We would like to padd all examples to the maximum length in the batch.\n",
    "But at the stage of preparing the example (in the `__getitem__` function), we do not know the batch neighbors!\n",
    "In order to change the batch merging logic, we need to write our own `collate_fn` function in the DataLoader constructor:\n",
    "\n",
    "```\n",
    "def collate_fn(samples):\n",
    "    # samples -- list of dictionaries samples\n",
    "    <...>\n",
    "    return batch\n",
    "```\n",
    "\n",
    "**(1 point)** Write a `collate_fn` function that padds _correctly_ rus and eng sequences and merges them into batches, where `batch[i, :]` returns the tokens for the `i` example.\n",
    "\n",
    "Expected output (for a sequence with left padding):\n",
    "\n",
    "```\n",
    "tensor([[   1,   10, 3429,  405,  113,  676,   10, 1031,  140,    4,    2],\n",
    "        [   0,    1,   57,   18,   23,   19,   61,    7,  140,    4,    2],\n",
    "        [   0,    0,    0,    1,   16,   17, 1131,  416,  140,    4,    2],\n",
    "        [   0,    0,    0,    1,   13,  465,   75,  197,  140,    4,    2],\n",
    "        [   0,    0,    0,    1,    6,  302,   13,  144,  140,    4,    2],\n",
    "        [   0,    1,    6,   59,  205,  167,    8,   15,  140,    4,    2],\n",
    "        [   0,    0,    0,    0,    1,    6,   14,  678,  140,    4,    2],\n",
    "        [   0,    0,    1,    5,   29,   67,    6,   14,  140,    4,    2]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T13:13:29.124773Z",
     "start_time": "2022-12-03T13:13:29.106769Z"
    },
    "id": "SMSeFFHQ8S3y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33667 8417\n"
     ]
    }
   ],
   "source": [
    "class EngRusCollate:\n",
    "    def __init__(self, padding_value: int = 0, batch_first: bool = True) -> None:\n",
    "        self.padding_value = padding_value\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "    def __call__(self, batch: List[dict]) -> Dict[str, torch.tensor]:\n",
    "        # getting max length\n",
    "        data = dict(\n",
    "                eng=list(),\n",
    "                rus=list()\n",
    "            )\n",
    "        \n",
    "        for seq in batch:\n",
    "            data[\"eng\"].append(torch.flip(seq.get(\"eng\"), dims=(0, )))\n",
    "            data[\"rus\"].append(seq.get(\"rus\"))\n",
    "        return {k: torch.flip(pad_sequence(v, batch_first=self.batch_first, \n",
    "                                           padding_value=self.padding_value), dims=(1, )) \\\n",
    "                if k ==\"eng\" else \\\n",
    "                   pad_sequence(v, batch_first=self.batch_first, \n",
    "                                padding_value=self.padding_value).to(torch.int64) for k, v in data.items()}\n",
    "    \n",
    "    \n",
    "trainset = EngRusDataset(pairs=train_pairs)\n",
    "evalset = EngRusDataset(pairs=eval_pairs)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=8, shuffle=True, collate_fn=EngRusCollate())\n",
    "evalloader = DataLoader(evalset, batch_size=8, shuffle=False, collate_fn=EngRusCollate())\n",
    "\n",
    "print(len(trainloader), len(evalloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T11:31:04.439788Z",
     "start_time": "2022-12-03T11:31:04.409744Z"
    }
   },
   "outputs": [],
   "source": [
    "eng, rus = next(iter(trainloader)).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T11:31:16.327163Z",
     "start_time": "2022-12-03T11:31:16.312130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,    0,    0,    1,    7,   34,  227, 4960,    4,    2],\n",
       "        [   0,    0,    0,    1,    5,  360,   56,    6,   20,  725,    4,    2],\n",
       "        [   0,    1,    5,   22,   11,   41,    5,   72,    8,   47,    4,    2],\n",
       "        [   0,    0,    0,    0,    1,    6,   59,  262,   66,  187,    4,    2],\n",
       "        [   1,   15,    7,   36,    8,   93,   23,    5,   51,  100,    9,    2],\n",
       "        [   0,    0,    1,   16,  388,   33, 1471,   85,   94,  303,    4,    2],\n",
       "        [   1,    5,  139,    6,   74,   19,   73,  233,   44,  123,    4,    2],\n",
       "        [   0,    0,    1,    6,   60,  126,   21,   12,  330,  370,    4,    2]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T13:17:45.946338Z",
     "start_time": "2022-12-03T13:17:45.929351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD PAD PAD PAD PAD SOS you are two faced . EOS\n",
      "PAD PAD PAD SOS i wonder why tom was fired . EOS\n",
      "PAD SOS i don t think i need to go . EOS\n",
      "PAD PAD PAD PAD SOS tom has lost all hope . EOS\n",
      "SOS do you want to see what i ve got ? EOS\n",
      "PAD PAD SOS it seems my dreams never come true . EOS\n",
      "SOS i asked tom where he had bought his car . EOS\n",
      "PAD PAD SOS tom will leave in a few days . EOS\n"
     ]
    }
   ],
   "source": [
    "for idx in range(eng.shape[0]):\n",
    "    sentence = eng[idx, :].tolist()\n",
    "    print(idx2sentence(seq=sentence, idx2tok=idx2eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T11:31:28.186086Z",
     "start_time": "2022-12-03T11:31:28.172589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,   12,    3,    4,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,  282,   45,   21, 1381,    4,    2,    0,    0,    0,    0],\n",
       "        [   1,    6,   40,    9,   15,   64,  157,    4,    2,    0,    0],\n",
       "        [   1,    7,  364, 8131, 2823,    4,    2,    0,    0,    0,    0],\n",
       "        [   1,  116, 2327,    9,   16,   22,   42,    8,    2,    0,    0],\n",
       "        [   1,  296,   79, 2885,   59,    6,    3,    4,    2,    0,    0],\n",
       "        [   1,    5,  295,   16,  313,   63,   18,  177,  178,    4,    2],\n",
       "        [   1,    7, 2987,  263,  193,  690,    4,    2,    0,    0,    0]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T13:17:11.929554Z",
     "start_time": "2022-12-03T13:17:11.909300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS ты UNK . EOS PAD PAD PAD PAD PAD PAD\n",
      "SOS интересно, почему тома уволили . EOS PAD PAD PAD PAD\n",
      "SOS не думаю, что мне нужно идти . EOS PAD PAD\n",
      "SOS том потерял всякую надежду . EOS PAD PAD PAD PAD\n",
      "SOS хочешь посмотреть, что у меня есть ? EOS PAD PAD\n",
      "SOS похоже, мои мечты никогда не UNK . EOS PAD PAD\n",
      "SOS я спросил у тома, где он купил машину . EOS\n",
      "SOS том уедет через несколько днеи . EOS PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "for idx in range(rus.shape[0]):\n",
    "    sentence = rus[idx, :].tolist()\n",
    "    print(idx2sentence(seq=sentence, idx2tok=idx2rus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDcGAXiA-mKm"
   },
   "source": [
    "Now we have the correct data generator, and all we have to do is write the model (encoder and decoder).\n",
    "\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The input tensor contains integers and has dimensions `[bs, seq_len]`,\n",
    "\n",
    "We will pass them through the layer with embeddings and get the tensor `[bs, seq_len, dim]`. Now we have floating point numbers that can be fed to RNN layers as input.\n",
    "\n",
    "\n",
    "\n",
    "GRU is an RNN with a specific structure:\n",
    "<img src=\"https://habrastorage.org/webt/xt/_q/nj/xt_qnjgfjengqoqd4gizkq4j_wk.png\">\n",
    "\n",
    "In the picture, the yellow rectangles are the line layers with the corresponding activation functions.\n",
    "\n",
    "\n",
    "`nn.RNN` allows you to create and use multi-layer one- and two-way layers as one layer.\n",
    "All parameters must be specified during creation, and then simply applied during the forward pass.\n",
    "\n",
    "\n",
    "The order of dimensions is a bit different from the usual in convolutional networks, this is due to the inability to parallel recurrent calculations effectively.\n",
    "\n",
    "\n",
    "**batch_first=True**\n",
    "\n",
    "Such an RNN layer expects two tensors as input:\n",
    "  - input with sizes `[bs, seq_len, dim]`,\n",
    "  - hidden_state with dimensions `[num_layers * num_directions, bs, hidden_size]`.\n",
    " \n",
    " \n",
    "The output is two tensors:\n",
    "- output `[bs, seq_len, dim]`,\n",
    "- hidden `[num_layers * num_directions, bs, hidden]`.\n",
    "\n",
    "We will apply RNN in two ways:\n",
    "- to the entire sequence, to translate the entire phrase in one language into one vector (EncoderRNN)\n",
    "- to one tensor and input token to generate a phrase in another language (DecoderRNN)\n",
    "\n",
    "\n",
    "We will put the entire input sequence into a hidden state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T08:27:21.200432Z",
     "start_time": "2022-12-03T08:27:21.146327Z"
    },
    "id": "dph7rI9_8S33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 12])\n",
      "torch.Size([8, 12, 256]) torch.Size([1, 8, 256])\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size: int, vocab_size: int, layers: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # initialize the embeddings with \"hidden_size\" size for each token in the vocab\n",
    "        # each token initialized from standard normal distribution N(0, 1)\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)\n",
    "        \n",
    "        # initialize RNN model with \"layers\" num layers of GRU cells. \"Bidirectional\" argument is not\n",
    "        # specified -> the model is one directional\n",
    "        self.rnn = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "    def forward(self, input_: torch.tensor, hidden: torch.tensor) -> Tuple[torch.tensor]:\n",
    "        # getting tensor of shape [batch_size, n_sequences, embedings size]\n",
    "        # it's X data to feed to GRU cell\n",
    "        embedded = self.embeddings(input_)\n",
    "        \n",
    "        # feed through rnn embedings and hidden\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size: int = 1, device: str = None) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Method initializes the first hidden vector with shape (D∗num_layers, N, H) to feed to first GRU cell.\n",
    "        D = 2 if bidirectional otherwise 1, N - batch size, H - hidden size.\n",
    "        All values are equal to 0\n",
    "        \"\"\"\n",
    "        # be aware about dimension! https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
    "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# initialize the encoder with hidden size 256\n",
    "enc = EncoderRNN(hidden_size=256, vocab_size=len(eng2idx))\n",
    "\n",
    "x = next(iter(trainloader))[\"eng\"]\n",
    "print(x.shape)\n",
    "\n",
    "# first hidden vector is a vector of zeroes\n",
    "# so, initialize this vector\n",
    "hidden = enc.init_hidden(batch_size=x.shape[0])\n",
    "output, hidden = enc(x, hidden)\n",
    "print(output.shape, hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg6cvr8L-mKm"
   },
   "source": [
    "We want the decoder to generate a translation for us -- a sequence of tokens from another language, using the encoder's hidden state vector.\n",
    "\n",
    "To do this, we will supply hidden and `<SOS>`token to the input.\n",
    "At each step, the decoder will return hidden and output vector.\n",
    "Output vector is the probability distribution for the next token (respectively, it has the size of the output language dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T11:39:09.106400Z",
     "start_time": "2022-12-03T11:39:09.094885Z"
    },
    "id": "oYfecQb88S38"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size: int, vocab_size: int, layers: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # initialize the embeddings with \"hidden_size\" size for each token in the vocab\n",
    "        # each token initialized from standard normal distribution N(0, 1)\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)\n",
    "        \n",
    "        # initialize onedirectional RNN of decoder with GRU cells\n",
    "        self.rnn = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        # finally we have to classify our tokens with Linear layer at the end\n",
    "        self.classifier = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input_: torch.tensor, hidden: torch.tensor) -> Tuple[torch.tensor]:\n",
    "        embedded = self.embeddings(input_)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.softmax(self.classifier(output))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size: int = 1, device: str = None) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Method initializes the first hidden vector with shape (D∗num_layers, N, H) to feed to first GRU cell.\n",
    "        D = 2 if bidirectional otherwise 1, N - batch size, H - hidden size.\n",
    "        All values are equal to 0\n",
    "        \"\"\"\n",
    "        # be aware about dimension! https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
    "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T11:39:20.725429Z",
     "start_time": "2022-12-03T11:39:20.682672Z"
    },
    "id": "oyMwF94n8S3_"
   },
   "outputs": [],
   "source": [
    "dec = DecoderRNN(hidden_size=256, vocab_size=len(rus2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Vj3_PB-mKn"
   },
   "source": [
    "Let's get a tensor with tokens of size `[bs, seq_len]` from the data generator and try to iterate over seq_len to generate the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T11:39:32.564852Z",
     "start_time": "2022-12-03T11:39:32.505379Z"
    },
    "id": "wVFcg0Uz8S4C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n",
      "torch.Size([8, 1, 10000]) torch.Size([1, 8, 256])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(trainloader))[\"rus\"] # get batch\n",
    "bs, seq_len = batch.shape\n",
    "hidden = dec.init_hidden(batch_size=bs)\n",
    "\n",
    "for i in range(0, seq_len):\n",
    "    step = batch[:, i].unsqueeze(1)  # get tokens sample for i-th step \n",
    "     # These are the correct tokens (ground truth), we could generate them\n",
    "     # unsqueeze adds dimension 1 (from [bs] to [bs, 1])\n",
    "    \n",
    "    output, hidden = dec(step, hidden)\n",
    "    print(output.shape, hidden.shape)\n",
    "    # output -- this is the probability distribution for the next token\n",
    "    # hidden -- this is the updated hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T12:57:14.974456Z",
     "start_time": "2022-12-03T12:57:14.960967Z"
    }
   },
   "outputs": [],
   "source": [
    "arr = torch.tensor([1, 2, 4], requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECBleoB4F58E"
   },
   "source": [
    "**(6 points)** Fill in a training part and train the encoder and decoder.\n",
    "\n",
    "1. You need to write getting the next token (integer) from the distribution: a vector of size `len(rus2idx)`. Since we are working in batches, this should be a batchified operation. You have several options for how to do this:\n",
    " - take by argmax\n",
    " - sample from distribution (torch.multinomial)\n",
    " - during training, take tokens from ground truth (and this must be done at least sometimes so that the model converges).\n",
    " \n",
    "2. You need to write a loss calculation. It is convenient to do this at each step: after the `<EOS>` occurs in the example, you do not need to count the loss for it (in the vectorized version, you can multiply the loss for `<PAD>`-tokens by zero - this is called masking). Loss is simply the sum of cross-entropy losses for each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T15:12:35.870342Z",
     "start_time": "2022-12-03T15:12:35.842346Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    ")\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def train_model(model: Tuple[nn.Module], optimizer: torch.optim, dataloader: DataLoader, device: str = None, \n",
    "                teacher_forcing_ratio: float = 0.5, verbose_step: int = 100, pad_token: int = 0) -> defaultdict: \n",
    "    LOGGER.info(\"\\tTrain loop\")\n",
    "    encoder, decoder = model\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    logs = defaultdict(list)\n",
    "    \n",
    "    # reduction False to use loss masking\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        eng = batch['eng'].to(device)\n",
    "        rus = batch['rus'].to(device)\n",
    "        \n",
    "        # rus sequence length and batch size\n",
    "        batch_size, rus_seq_length = rus.shape\n",
    "        \n",
    "        encoder_hidden = encoder.init_hidden(batch_size=batch_size, device=device)\n",
    "        _, hidden = encoder(eng, encoder_hidden)\n",
    "        \n",
    "        # write decoder application and loss calculation.\n",
    "        # hint: loss must be masked, in case the sequence has already ended.\n",
    "              \n",
    "        # the first token for the decoder input is <SOS>\n",
    "        decoder_input = rus[:, 0].unsqueeze(1)\n",
    "        \n",
    "        loss = 0\n",
    "        for token in range(1, rus_seq_length):\n",
    "            # forward single decoder GRU cell\n",
    "            decoder_outputs, hidden = decoder(decoder_input, hidden)\n",
    "            \n",
    "            # now we have to decide, which token should we use for the next GRU cell of the decoder:\n",
    "            # token from target rus sequence or output from the previous GRU cell\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            decoder_input = rus[:, token].unsqueeze(1) if teacher_force else decoder_outputs.argmax(2)\n",
    "            \n",
    "            curr_loss = criterion(decoder_outputs.view(batch_size, -1), rus[:, token])\n",
    "            curr_loss = torch.masked_select(curr_loss, rus[:, token] != pad_token).mean()\n",
    "            loss += curr_loss.mean()\n",
    "        \n",
    "        loss = loss / (rus_seq_length - 1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.cpu().item()\n",
    "        logs[\"loss\"].append(loss)\n",
    "        \n",
    "        if verbose_step is not None and not i % verbose_step:\n",
    "            LOGGER.info(\"\\tIteration [{}]/[{}] loss: {:.6f}\".format(i, len(dataloader), loss))\n",
    "            \n",
    "    return logs, (encoder, decoder)\n",
    "\n",
    "def eval_model(model: Tuple[nn.Module], dataloader: DataLoader, device: str = None, \n",
    "               verbose_step: int = 100, pad_token: int = 0):\n",
    "    LOGGER.info(\"\\tEval loop\")\n",
    "    encoder, decoder = model\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    logs = defaultdict(list)\n",
    "    \n",
    "    # reduction False to use loss masking\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        eng = batch['eng'].to(device)\n",
    "        rus = batch['rus'].to(device)\n",
    "\n",
    "        # rus sequence length and batch size\n",
    "        batch_size, rus_seq_length = rus.shape\n",
    "\n",
    "        encoder_hidden = encoder.init_hidden(batch_size=batch_size, device=device)\n",
    "        _, hidden = encoder(eng, encoder_hidden)\n",
    "\n",
    "        # the first token for the decoder input is <SOS>\n",
    "        decoder_input = rus[:, 0].unsqueeze(1)\n",
    "\n",
    "        loss = 0\n",
    "        for token in range(1, rus_seq_length):\n",
    "            # forward single decoder GRU cell\n",
    "            decoder_outputs, hidden = decoder(decoder_input, hidden)\n",
    "            decoder_input = decoder_outputs.argmax(2)\n",
    "\n",
    "            curr_loss = criterion(decoder_outputs.view(batch_size, -1), rus[:, token])\n",
    "            curr_loss = torch.masked_select(curr_loss, rus[:, token] != pad_token).mean()\n",
    "            loss += curr_loss.mean()\n",
    "\n",
    "        loss = loss / (rus_seq_length - 1)\n",
    "        loss = loss.cpu().item()\n",
    "        logs[\"loss\"].append(loss)\n",
    "\n",
    "        if verbose_step is not None and not i % verbose_step:\n",
    "            LOGGER.info(\"\\tIteration [{}]/[{}] loss: {:.6f}\".format(i, len(dataloader), loss))\n",
    "                \n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T16:52:44.667587Z",
     "start_time": "2022-12-03T15:13:14.453786Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 18:13:14,507] {<ipython-input-400-30d0786d864b>:24} INFO - Iterate over 5 epochs\n",
      "[2022-12-03 18:13:14,508] {<ipython-input-400-30d0786d864b>:26} INFO - Epoch [1]/[5]\n",
      "[2022-12-03 18:13:14,509] {<ipython-input-399-3ee10b9b7cd5>:10} INFO - \tTrain loop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d825fe522c47059c3ec9b59986a623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 18:13:15,099] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [0]/[2105] loss: 9.235302\n",
      "[2022-12-03 18:13:31,513] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [150]/[2105] loss: 4.333659\n",
      "[2022-12-03 18:13:47,906] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [300]/[2105] loss: 3.989600\n",
      "[2022-12-03 18:14:05,299] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [450]/[2105] loss: 4.219264\n",
      "[2022-12-03 18:14:31,480] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [600]/[2105] loss: 3.749774\n",
      "[2022-12-03 18:14:52,836] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [750]/[2105] loss: 3.912883\n",
      "[2022-12-03 18:15:33,419] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [900]/[2105] loss: 4.104872\n",
      "[2022-12-03 18:16:42,558] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1050]/[2105] loss: 3.895092\n",
      "[2022-12-03 18:17:54,791] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1200]/[2105] loss: 4.870035\n",
      "[2022-12-03 18:19:21,937] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1350]/[2105] loss: 4.993133\n",
      "[2022-12-03 18:20:34,228] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1500]/[2105] loss: 5.471542\n",
      "[2022-12-03 18:21:57,855] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1650]/[2105] loss: 3.976876\n",
      "[2022-12-03 18:23:20,597] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1800]/[2105] loss: 5.642423\n",
      "[2022-12-03 18:24:38,841] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1950]/[2105] loss: 4.797594\n",
      "[2022-12-03 18:26:05,203] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [2100]/[2105] loss: 4.196367\n",
      "[2022-12-03 18:26:05,699] {<ipython-input-399-3ee10b9b7cd5>:67} INFO - \tEval loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f87deb3ed7e42dcb9265fac809dd684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=527.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 18:26:05,764] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [0]/[527] loss: 4.781900\n",
      "[2022-12-03 18:26:36,980] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [150]/[527] loss: 6.159809\n",
      "[2022-12-03 18:27:05,292] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [300]/[527] loss: 4.737787\n",
      "[2022-12-03 18:27:35,925] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [450]/[527] loss: 4.768664\n",
      "[2022-12-03 18:27:49,610] {<ipython-input-400-30d0786d864b>:41} INFO - Train loss avg:\t4.253991\n",
      "[2022-12-03 18:27:49,611] {<ipython-input-400-30d0786d864b>:42} INFO - Eval loss avg:\t4.952376\n",
      "[2022-12-03 18:27:49,612] {<ipython-input-400-30d0786d864b>:26} INFO - Epoch [2]/[5]\n",
      "[2022-12-03 18:27:49,613] {<ipython-input-399-3ee10b9b7cd5>:10} INFO - \tTrain loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82e97efd4d04c5e9b68096bc9bec0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 18:27:50,374] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [0]/[2105] loss: 4.212917\n",
      "[2022-12-03 18:29:16,902] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [150]/[2105] loss: 4.220631\n",
      "[2022-12-03 18:30:41,022] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [300]/[2105] loss: 4.123466\n",
      "[2022-12-03 18:32:04,142] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [450]/[2105] loss: 4.373175\n",
      "[2022-12-03 18:33:25,878] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [600]/[2105] loss: 3.183547\n",
      "[2022-12-03 18:34:41,783] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [750]/[2105] loss: 3.580933\n",
      "[2022-12-03 18:36:03,200] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [900]/[2105] loss: 4.275609\n",
      "[2022-12-03 18:37:27,693] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1050]/[2105] loss: 4.549936\n",
      "[2022-12-03 18:38:53,760] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1200]/[2105] loss: 4.319018\n",
      "[2022-12-03 18:40:13,569] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1350]/[2105] loss: 4.072594\n",
      "[2022-12-03 18:41:35,751] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1500]/[2105] loss: 4.745621\n",
      "[2022-12-03 18:43:04,647] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1650]/[2105] loss: 4.645039\n",
      "[2022-12-03 18:44:21,480] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1800]/[2105] loss: 5.070407\n",
      "[2022-12-03 18:45:43,806] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1950]/[2105] loss: 4.089727\n",
      "[2022-12-03 18:47:15,576] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [2100]/[2105] loss: 3.826879\n",
      "[2022-12-03 18:47:16,132] {<ipython-input-399-3ee10b9b7cd5>:67} INFO - \tEval loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44be7a3eb714ee2af141f541983f7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=527.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 18:47:16,205] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [0]/[527] loss: 4.836901\n",
      "[2022-12-03 18:47:42,474] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [150]/[527] loss: 5.357322\n",
      "[2022-12-03 18:48:15,495] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [300]/[527] loss: 5.324896\n",
      "[2022-12-03 18:48:44,114] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [450]/[527] loss: 4.822260\n",
      "[2022-12-03 18:49:01,500] {<ipython-input-400-30d0786d864b>:41} INFO - Train loss avg:\t4.135034\n",
      "[2022-12-03 18:49:01,500] {<ipython-input-400-30d0786d864b>:42} INFO - Eval loss avg:\t5.043538\n",
      "[2022-12-03 18:49:01,501] {<ipython-input-400-30d0786d864b>:26} INFO - Epoch [3]/[5]\n",
      "[2022-12-03 18:49:01,501] {<ipython-input-399-3ee10b9b7cd5>:10} INFO - \tTrain loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c08986be78400895bbfc27ed561ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 18:49:02,249] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [0]/[2105] loss: 3.967948\n",
      "[2022-12-03 18:50:26,212] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [150]/[2105] loss: 3.975446\n",
      "[2022-12-03 18:51:51,377] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [300]/[2105] loss: 3.506905\n",
      "[2022-12-03 18:53:14,052] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [450]/[2105] loss: 4.195498\n",
      "[2022-12-03 18:54:40,377] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [600]/[2105] loss: 3.729006\n",
      "[2022-12-03 18:56:06,580] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [750]/[2105] loss: 3.630001\n",
      "[2022-12-03 18:57:26,560] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [900]/[2105] loss: 3.535708\n",
      "[2022-12-03 18:58:48,561] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1050]/[2105] loss: 4.804193\n",
      "[2022-12-03 19:00:18,761] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1200]/[2105] loss: 3.431057\n",
      "[2022-12-03 19:01:40,916] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1350]/[2105] loss: 3.660678\n",
      "[2022-12-03 19:03:08,438] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1500]/[2105] loss: 3.693430\n",
      "[2022-12-03 19:04:33,909] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1650]/[2105] loss: 4.113639\n",
      "[2022-12-03 19:05:57,313] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1800]/[2105] loss: 4.338427\n",
      "[2022-12-03 19:07:16,702] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1950]/[2105] loss: 3.265095\n",
      "[2022-12-03 19:08:47,088] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [2100]/[2105] loss: 4.479534\n",
      "[2022-12-03 19:08:48,332] {<ipython-input-399-3ee10b9b7cd5>:67} INFO - \tEval loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a5005859204ae49fc27ef4afb30c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=527.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 19:08:48,407] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [0]/[527] loss: 5.418152\n",
      "[2022-12-03 19:09:14,460] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [150]/[527] loss: 5.640875\n",
      "[2022-12-03 19:09:44,893] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [300]/[527] loss: 5.529169\n",
      "[2022-12-03 19:10:16,334] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [450]/[527] loss: 5.150251\n",
      "[2022-12-03 19:10:30,369] {<ipython-input-400-30d0786d864b>:41} INFO - Train loss avg:\t4.065553\n",
      "[2022-12-03 19:10:30,369] {<ipython-input-400-30d0786d864b>:42} INFO - Eval loss avg:\t5.455361\n",
      "[2022-12-03 19:10:30,370] {<ipython-input-400-30d0786d864b>:26} INFO - Epoch [4]/[5]\n",
      "[2022-12-03 19:10:30,370] {<ipython-input-399-3ee10b9b7cd5>:10} INFO - \tTrain loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ebe3013e9045fe9c2161e59cf38f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 19:10:31,055] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [0]/[2105] loss: 4.341008\n",
      "[2022-12-03 19:11:55,176] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [150]/[2105] loss: 3.420315\n",
      "[2022-12-03 19:13:24,629] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [300]/[2105] loss: 3.673876\n",
      "[2022-12-03 19:14:39,461] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [450]/[2105] loss: 3.882543\n",
      "[2022-12-03 19:16:03,918] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [600]/[2105] loss: 3.937642\n",
      "[2022-12-03 19:17:35,038] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [750]/[2105] loss: 3.630076\n",
      "[2022-12-03 19:18:57,440] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [900]/[2105] loss: 3.795650\n",
      "[2022-12-03 19:20:16,840] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1050]/[2105] loss: 3.844103\n",
      "[2022-12-03 19:21:44,808] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1200]/[2105] loss: 4.153373\n",
      "[2022-12-03 19:23:04,962] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1350]/[2105] loss: 3.737013\n",
      "[2022-12-03 19:24:25,702] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1500]/[2105] loss: 4.606944\n",
      "[2022-12-03 19:25:47,344] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1650]/[2105] loss: 3.526365\n",
      "[2022-12-03 19:27:18,446] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1800]/[2105] loss: 3.304421\n",
      "[2022-12-03 19:28:28,373] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1950]/[2105] loss: 4.415572\n",
      "[2022-12-03 19:29:45,745] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [2100]/[2105] loss: 3.824941\n",
      "[2022-12-03 19:29:47,763] {<ipython-input-399-3ee10b9b7cd5>:67} INFO - \tEval loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6141c53408f4003ae3cb7a68f8987b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=527.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 19:29:47,977] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [0]/[527] loss: 4.497373\n",
      "[2022-12-03 19:30:22,028] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [150]/[527] loss: 5.815016\n",
      "[2022-12-03 19:30:50,630] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [300]/[527] loss: 4.584311\n",
      "[2022-12-03 19:31:19,783] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [450]/[527] loss: 4.776179\n",
      "[2022-12-03 19:31:36,207] {<ipython-input-400-30d0786d864b>:41} INFO - Train loss avg:\t4.008509\n",
      "[2022-12-03 19:31:36,208] {<ipython-input-400-30d0786d864b>:42} INFO - Eval loss avg:\t4.964145\n",
      "[2022-12-03 19:31:36,209] {<ipython-input-400-30d0786d864b>:26} INFO - Epoch [5]/[5]\n",
      "[2022-12-03 19:31:36,210] {<ipython-input-399-3ee10b9b7cd5>:10} INFO - \tTrain loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66530e7122144ad789c0d75fa9108d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 19:31:36,976] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [0]/[2105] loss: 3.818311\n",
      "[2022-12-03 19:33:02,620] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [150]/[2105] loss: 4.165160\n",
      "[2022-12-03 19:34:27,990] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [300]/[2105] loss: 3.947241\n",
      "[2022-12-03 19:35:45,081] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [450]/[2105] loss: 3.797295\n",
      "[2022-12-03 19:37:10,302] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [600]/[2105] loss: 3.383263\n",
      "[2022-12-03 19:38:27,680] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [750]/[2105] loss: 3.795603\n",
      "[2022-12-03 19:39:56,061] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [900]/[2105] loss: 4.948834\n",
      "[2022-12-03 19:41:18,444] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1050]/[2105] loss: 4.052854\n",
      "[2022-12-03 19:42:41,785] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1200]/[2105] loss: 3.354173\n",
      "[2022-12-03 19:43:58,405] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1350]/[2105] loss: 3.740671\n",
      "[2022-12-03 19:45:19,058] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1500]/[2105] loss: 2.963092\n",
      "[2022-12-03 19:46:45,736] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1650]/[2105] loss: 4.109571\n",
      "[2022-12-03 19:48:12,306] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1800]/[2105] loss: 3.870993\n",
      "[2022-12-03 19:49:34,790] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [1950]/[2105] loss: 5.368693\n",
      "[2022-12-03 19:50:52,525] {<ipython-input-399-3ee10b9b7cd5>:61} INFO - \tIteration [2100]/[2105] loss: 3.593003\n",
      "[2022-12-03 19:50:54,417] {<ipython-input-399-3ee10b9b7cd5>:67} INFO - \tEval loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1dff49ca76489d9583fdf60d066a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=527.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03 19:50:54,546] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [0]/[527] loss: 4.628288\n",
      "[2022-12-03 19:51:26,547] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [150]/[527] loss: 5.936299\n",
      "[2022-12-03 19:52:00,938] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [300]/[527] loss: 4.528725\n",
      "[2022-12-03 19:52:26,714] {<ipython-input-399-3ee10b9b7cd5>:107} INFO - \tIteration [450]/[527] loss: 4.480340\n",
      "[2022-12-03 19:52:44,657] {<ipython-input-400-30d0786d864b>:41} INFO - Train loss avg:\t3.914209\n",
      "[2022-12-03 19:52:44,658] {<ipython-input-400-30d0786d864b>:42} INFO - Eval loss avg:\t4.926310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-2\n",
    "BATCH_SIZE = 128\n",
    "HIDEN_SIZE = 256\n",
    "N_EPOCHS = 5\n",
    "WRITER_PATH = \"./runs/\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# initialize encoder & decoder\n",
    "encoder = EncoderRNN(hidden_size=HIDEN_SIZE, vocab_size=len(eng2idx))\n",
    "decoder = DecoderRNN(hidden_size=HIDEN_SIZE, vocab_size=len(rus2idx))\n",
    "\n",
    "# initialize optimizer\n",
    "opt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=LR)\n",
    "\n",
    "# initialize writer for tensorboard\n",
    "writer = SummaryWriter(WRITER_PATH)\n",
    "\n",
    "# initialize DataLoaders\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=EngRusCollate())\n",
    "evalloader = DataLoader(evalset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=EngRusCollate())\n",
    "\n",
    "# initialize the model and train it\n",
    "model = (encoder, decoder)\n",
    "LOGGER.info(\"Iterate over {} epochs\".format(N_EPOCHS))\n",
    "for epoch in range(N_EPOCHS):\n",
    "    LOGGER.info(\"Epoch [{}]/[{}]\".format(epoch + 1, N_EPOCHS))\n",
    "    train_logs, model = \\\n",
    "        train_model(model=model, optimizer=opt, dataloader=trainloader, device=device, teacher_forcing_ratio=0.5, \n",
    "                    verbose_step=150)\n",
    "    with torch.no_grad():\n",
    "        eval_logs = eval_model(model=model, dataloader=evalloader, device=device, verbose_step=150)\n",
    "    \n",
    "    train_loss_avg, eval_loss_avg = \\\n",
    "        np.mean(train_logs.get(\"loss\", 0)), np.mean(eval_logs.get(\"loss\", 0))\n",
    "    \n",
    "    writer.add_scalar(tag=os.path.join(WRITER_PATH, \"Loss\", \"Train\"), \n",
    "                      scalar_value=train_loss_avg, global_step=epoch + 1)\n",
    "    writer.add_scalar(tag=os.path.join(WRITER_PATH, \"Eval\", \"Train\"), \n",
    "                      scalar_value=eval_loss_avg, global_step=epoch + 1)\n",
    "    \n",
    "    LOGGER.info(\"Train loss avg:\\t{:.6f}\".format(train_loss_avg))\n",
    "    LOGGER.info(\"Eval loss avg:\\t{:.6f}\".format(eval_loss_avg))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8G9EoojGDdJ"
   },
   "source": [
    "**(2 points)** Write a translation function with sampling from a distribution with temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T19:28:22.050051Z",
     "start_time": "2022-12-03T19:28:19.560159Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder, decoder = model\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "encoder = encoder.to(\"cpu\")\n",
    "decoder = decoder.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T20:08:56.551619Z",
     "start_time": "2022-12-03T20:08:56.479587Z"
    },
    "id": "64h3_w528S4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 23, 14, 63, 46, 9, 2]\n",
      "чем чем чем чем чем чем чем чем чем чем\n",
      "замечательныи замечательныи замечательныи замечательныи замечательныи замечательныи замечательныи замечательныи замечательныи замечательныи\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      "? ? ? ? ? ? ? ? ? ?\n",
      "EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model: Tuple[nn.Module], sentence: str, eng2idx: Dict[str, int], idx2rus: Dict[str, int], \n",
    "             temp: Union[int, float] = 1.0, max_seq_length: int = 20, batch_size: int = 10, \n",
    "             sos_token: int = 1) -> None:\n",
    "    encoder, decoder = model\n",
    "    \n",
    "    # encode sentence to token sequence\n",
    "    encoded = sentence2idx(seq=sentence, tok2idx=eng2idx)\n",
    "    print(encoded)\n",
    "    \n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        # repeat encoded sentence to batch_size times\n",
    "        z = torch.LongTensor(encoded).view(1, -1).repeat(batch_size, 1)\n",
    "        \n",
    "        # feed forward throught encoder\n",
    "        encoder_outputs, hidden = encoder(z, encoder.init_hidden(batch_size=batch_size))\n",
    "        \n",
    "        # iterate over decoder till you will achive max sequence length\n",
    "        decoder_input = torch.full(size=(batch_size, 1), fill_value=sos_token)\n",
    "        for i in range(max_seq_length):\n",
    "            decoder_outputs, hidden = decoder(decoder_input, hidden)\n",
    "            decoder_input = F.softmax(decoder_outputs / temp, dim=2)\n",
    "            decoder_input = decoder_input.argmax(dim=2)\n",
    "            output.append(decoder_input.numpy())\n",
    "    \n",
    "    output = np.concatenate(output, axis=1).T\n",
    "    for s in output:\n",
    "        out = idx2sentence(s, idx2rus)\n",
    "        print(out.replace(\"PAD\", \"\"))\n",
    "\n",
    "    \n",
    "evaluate(model=(encoder, decoder), sentence=\"What is going on?\", eng2idx=eng2idx, idx2rus=idx2rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
